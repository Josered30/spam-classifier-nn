{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import datetime\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "import copy\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from heapq import nlargest\n",
    "from nltk.corpus import stopwords \n",
    "from collections import OrderedDict\n",
    "from itertools import islice\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "source": [
    "# Neural Network spam classifier\n",
    "In order to classify messages as spam or ham, we are going to use a neural network. This model have one input, hidden and output layer, and the sigmoid function as activation function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate=0.1):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.weights_input_hidden = np.random.uniform(-1,1,size=(hidden_nodes, input_nodes))\n",
    "        self.weights_hidden_output = np.random.uniform(-1,1,size=(output_nodes, hidden_nodes))\n",
    "        self.bias_hidden = np.ones((hidden_nodes,1))\n",
    "        self.bias_output = np.ones((output_nodes,1))\n",
    "        self.learning_rate=learning_rate\n",
    "        self.errors = np.empty((0))\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+ math.exp(-x))\n",
    "    \n",
    "    def derivate(self,x):\n",
    "        return x*(1-x)\n",
    "    \n",
    "   \n",
    "    def feedforward(self,input_v):\n",
    "        sigmoid_vector = np.vectorize(self.sigmoid)\n",
    "        \n",
    "        input_vector = input_v.reshape((self.input_nodes,1))\n",
    "    \n",
    "        hidden = self.weights_input_hidden@input_vector\n",
    "        hidden = np.add(hidden, self.bias_hidden)\n",
    "        hidden = sigmoid_vector(hidden)\n",
    "    \n",
    "        output = self.weights_hidden_output@hidden\n",
    "        output = np.add(output, self.bias_output)\n",
    "        output = sigmoid_vector(output)\n",
    "    \n",
    "        return output\n",
    "    \n",
    "    def backpropagation(self, input_v, target_v):\n",
    "        input_vector = input_v.reshape((self.input_nodes,1))\n",
    "        target_vector = target_v.reshape((self.output_nodes,1))\n",
    "        \n",
    "        sigmoid_vector = np.vectorize(self.sigmoid)\n",
    "        derivate_vector = np.vectorize(self.derivate)\n",
    "    \n",
    "        hidden = self.weights_input_hidden@input_vector\n",
    "        hidden = np.add(hidden, self.bias_hidden)\n",
    "        hidden = sigmoid_vector(hidden)\n",
    "        \n",
    "        output = self.weights_hidden_output@hidden\n",
    "        output = np.add(output, self.bias_output)\n",
    "        output = sigmoid_vector(output)\n",
    "        \n",
    "        \n",
    "        output_error = np.subtract(target_vector,output)\n",
    "        error = output_error.sum(0)\n",
    "        \n",
    "    \n",
    "        gradient = derivate_vector(output)\n",
    "        gradient = np.multiply(gradient,output_error)\n",
    "        gradient = np.multiply(gradient, self.learning_rate)\n",
    "        \n",
    "        hidden_transpose = np.transpose(hidden)\n",
    "        weights_ho_deltas = gradient@hidden_transpose\n",
    "        \n",
    "        self.weights_hidden_output = np.add(self.weights_hidden_output, weights_ho_deltas)\n",
    "        self.bias_output = np.add(self.bias_output, gradient)\n",
    "        \n",
    "        \n",
    "        transpose_weights_hidden_output = np.transpose(self.weights_hidden_output)\n",
    "        hidden_error = transpose_weights_hidden_output@output_error\n",
    "        \n",
    "    \n",
    "        hidden_gradient = derivate_vector(hidden)\n",
    "        hidden_gradient = np.multiply(hidden_gradient, hidden_error)\n",
    "        hidden_gradient = np.multiply(hidden_gradient, self.learning_rate)\n",
    "        \n",
    "        input_transpose = np.transpose(input_vector)\n",
    "        weights_ih_deltas = hidden_gradient@input_transpose\n",
    "        \n",
    "        self.weights_input_hidden = np.add(self.weights_input_hidden, weights_ih_deltas)\n",
    "        self.bias_hidden = np.add(self.bias_hidden, hidden_gradient)\n",
    "\n",
    "        return error\n",
    "        \n",
    "    \n",
    "    def train(self, train_dataframe, epochs):\n",
    "        spam = 0\n",
    "        ham = 0\n",
    "        iteration = 0\n",
    "        error_sample = 200\n",
    "        errors = []\n",
    "\n",
    "        for i in tqdm(range(epochs)):\n",
    "            print(\" Epoch\", i)\n",
    "            for index, row in train_dataframe.iterrows():\n",
    "                spam+=(row['label_tag'])  \n",
    "                input_v = row.to_numpy()\n",
    "                input_v = input_v[1:len(input_v)-1]\n",
    "                target_v = np.array([row['label_tag']])\n",
    "                error = self.backpropagation(input_v, target_v)    \n",
    "                if iteration%error_sample == 0:\n",
    "                    errors.append(error)\n",
    "                iteration += 1\n",
    "            \n",
    "        ham = (len(train_dataframe)*epochs)-spam\n",
    "        print(f\"Spam:{spam} - Ham:{ham}\")\n",
    "        print(\"Done\")  \n",
    "\n",
    "        return np.array(errors)"
   ]
  },
  {
   "source": [
    "### These are utils class for parsing, reading and saving data from the datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtil:\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_data(message):\n",
    "        message = re.sub(r\"\\$[\\d]+\",'price',message)\n",
    "        message = re.sub(r\"\\%[\\d]+\",'percentage',message)\n",
    "        message = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",'url',message)\n",
    "        message = re.sub(r\"www.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",'url',message)\n",
    "        message = re.sub(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\",'email',message)\n",
    "        message = re.sub(r'[\\W\\d]',' ',message)\n",
    "        message = re.sub(r'[\\s+]',' ',message)\n",
    "        message.strip()\n",
    "        return message\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_data(message):\n",
    "        message = message.lower() \n",
    "        message = DataUtil.normalize_data(message)\n",
    "        words = nltk.word_tokenize(message)\n",
    "\n",
    "        result = []\n",
    "        for word in words:\n",
    "            if word not in DataUtil.stop_words and len(word)>2:   \n",
    "                #words = DataUtil.stemmer.stem(words[i])\n",
    "                word = DataUtil.lemmatizer.lemmatize(word)\n",
    "                result.append(word)  \n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def order_and_take(data, key, n=None):\n",
    "        data = OrderedDict(sorted(data.items(), key=lambda i: i[1][key], reverse=True))\n",
    "        if n!=None:\n",
    "            data = dict(islice(data.items(), n))\n",
    "        return data\n",
    "\n",
    "\n",
    "class DocumentReader:\n",
    "    def __init__(self, document):\n",
    "        self.document = document\n",
    "        self.words_data = {}\n",
    "\n",
    "    def get_words(self):\n",
    "        df = pd.read_csv(self.document)\n",
    "        words_list = dict()\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            words = DataUtil.clean_data(row['message'])\n",
    "            for word in words: \n",
    "                if word not in words_list.keys():\n",
    "                    words_list[word] = 1\n",
    "                else:\n",
    "                    words_list[word] += 1\n",
    "        \n",
    "        result = { key:val for key, val in words_list.items() if val > 10}\n",
    "        result = nlargest(3000, result, key=result.get)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    @staticmethod\n",
    "    def tf(sentences):    \n",
    "        words_counter = {}\n",
    "        for index, sentence in enumerate(sentences):\n",
    "            words = DataUtil.clean_data(sentence)\n",
    "            for word in words: \n",
    "                    if word not in words_counter.keys(): \n",
    "                        words_counter[word] = {}\n",
    "                        words_counter[word]['sentences'] = {}\n",
    "                    if index not in words_counter[word]['sentences'].keys():\n",
    "                        words_counter[word]['sentences'][index] = 1/len(words)         \n",
    "                    else:\n",
    "                        words_counter[word]['sentences'][index] += 1/len(words)\n",
    "        return words_counter\n",
    "\n",
    "    @staticmethod\n",
    "    def tf_idf(message):\n",
    "        sentences = nltk.sent_tokenize(message)\n",
    "        words_count = Data.tf(sentences)\n",
    "        words_data = {}\n",
    "\n",
    "        for key, element in words_count.items():\n",
    "            words_data[key] = [0 for i in range(len(sentences))]\n",
    "            idf = math.log(len(sentences)/len(element['sentences']))\n",
    "            for index, sentence_ratio in element['sentences'].items():    \n",
    "                words_data[key][index] = sentence_ratio * idf\n",
    "        return words_data\n",
    "          \n",
    "    @staticmethod\n",
    "    def get_inputs_count(message, words_list):  \n",
    "        words = DataUtil.clean_data(message)  \n",
    "        inputs = np.zeros(len(words_list))\n",
    "\n",
    "        for index, word in enumerate(words_list):\n",
    "            if word in words:\n",
    "                inputs[index] +=1 \n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def load_unique_words(dataframe):\n",
    "        unique_words = {}\n",
    "        for index,row in dataframe.iterrows():\n",
    "            unique_words[row['word']] = 0\n",
    "        return unique_words\n"
   ]
  },
  {
   "source": [
    "First of all we need to get all the unique words from our dataset, so we parse each message into tokens and keep the most frequent words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         word\n",
       "0        call\n",
       "1         get\n",
       "2         day\n",
       "3        free\n",
       "4        know\n",
       "..        ...\n",
       "755  discount\n",
       "756       std\n",
       "757   dogging\n",
       "758       hmv\n",
       "759   fantasy\n",
       "\n",
       "[760 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>call</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>get</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>free</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>know</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>755</th>\n      <td>discount</td>\n    </tr>\n    <tr>\n      <th>756</th>\n      <td>std</td>\n    </tr>\n    <tr>\n      <th>757</th>\n      <td>dogging</td>\n    </tr>\n    <tr>\n      <th>758</th>\n      <td>hmv</td>\n    </tr>\n    <tr>\n      <th>759</th>\n      <td>fantasy</td>\n    </tr>\n  </tbody>\n</table>\n<p>760 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#ham_document_reader = DocumentReader('ham.csv')\n",
    "#spam_document_reader = DocumentReader('spam.csv')\n",
    "spam_ham_document_reader = DocumentReader('spamham.csv')\n",
    "\n",
    "unique_words = spam_ham_document_reader.get_words()\n",
    "unique_words_df = pd.DataFrame(unique_words, columns=['word'])\n",
    "unique_words_df.to_csv('words.csv')\n",
    "\n",
    "unique_words_df"
   ]
  },
  {
   "source": [
    "Then we load the full dataset and split it into the train and test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  class                                            message  label_tag\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0\n",
       "1   ham                      Ok lar... Joking wif u oni...          0\n",
       "2   ham  U dun say so early hor... U c already then say...          0\n",
       "3   ham  Nah I don't think he goes to usf, he lives aro...          0\n",
       "4   ham  Even my brother is not like to speak with me. ...          0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>message</th>\n      <th>label_tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df = pd.read_csv('spamham.csv')\n",
    "df[\"label_tag\"] = df[\"class\"].map({'ham':0, 'spam':1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     class                                            message  label_tag\n",
       "2235   ham  Lol I have to take it. member how I said my au...          0\n",
       "4803   ham                 No, I was trying it all weekend ;V          0\n",
       "3073   ham  SO IS TH GOWER MATE WHICH IS WHERE I AM!?! HOW...          0\n",
       "5293  spam  Wan2 win a Meet+Greet with Westlife 4 U or a m...          1\n",
       "4113   ham                 Ok lor... Or u wan me go look 4 u?          0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>message</th>\n      <th>label_tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2235</th>\n      <td>ham</td>\n      <td>Lol I have to take it. member how I said my au...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4803</th>\n      <td>ham</td>\n      <td>No, I was trying it all weekend ;V</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3073</th>\n      <td>ham</td>\n      <td>SO IS TH GOWER MATE WHICH IS WHERE I AM!?! HOW...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5293</th>\n      <td>spam</td>\n      <td>Wan2 win a Meet+Greet with Westlife 4 U or a m...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4113</th>\n      <td>ham</td>\n      <td>Ok lor... Or u wan me go look 4 u?</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train_set = df.sample(frac=0.8)\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   class                                            message  label_tag\n",
       "6    ham  I'm gonna be home soon and i don't want to tal...          0\n",
       "13   ham  I???m going to try for 2 months ha ha only joking          0\n",
       "16   ham  Ffffffffff. Alright no way I can meet up with ...          0\n",
       "33   ham                                WHO ARE YOU SEEING?          0\n",
       "39   ham  U don't know how stubborn I am. I didn't even ...          0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>message</th>\n      <th>label_tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>ham</td>\n      <td>I'm gonna be home soon and i don't want to tal...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>ham</td>\n      <td>I???m going to try for 2 months ha ha only joking</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>ham</td>\n      <td>Ffffffffff. Alright no way I can meet up with ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>ham</td>\n      <td>WHO ARE YOU SEEING?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>ham</td>\n      <td>U don't know how stubborn I am. I didn't even ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "test_set = df.drop(train_set.index)\n",
    "test_set.head()"
   ]
  },
  {
   "source": [
    "Now we get the inputs for all the train set. Each input is an array of lenght N, where N is the number of unique words in out dataset and its value, the frequency of each word in the message"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_set = np.zeros((len(train_set),len(unique_words)+1))\n",
    "iter = 0\n",
    "\n",
    "for index, row in train_set.iterrows():\n",
    "    inputs = Data.get_inputs_count(row['message'],unique_words)\n",
    "    inputs = np.append(inputs, int(row['label_tag']))\n",
    "    train_data_set[iter] = inputs\n",
    "    iter+=1\n",
    "\n",
    "indexes = [i for i in range(iter)]\n",
    "columns = copy.deepcopy(unique_words)\n",
    "columns.append('label_tag')\n",
    "\n",
    "train_dataframe = pd.DataFrame(data=train_data_set, index = indexes, columns= columns)\n",
    "train_dataframe.to_csv('train_set.csv')"
   ]
  },
  {
   "source": [
    "Now we create a neural network with input, hidden and output layer of size N, N+1//2 and 1 size. Each unique word will be represented as an element in the input layer and we expect an unique result (1 spam - 0 ham).\n",
    "\n",
    "- N is the number of unique words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s] Epoch 0\n",
      "  5%|▌         | 1/20 [00:08<02:32,  8.04s/it] Epoch 1\n",
      " 10%|█         | 2/20 [00:16<02:29,  8.28s/it] Epoch 2\n",
      " 15%|█▌        | 3/20 [00:25<02:24,  8.50s/it] Epoch 3\n",
      " 20%|██        | 4/20 [00:34<02:17,  8.62s/it] Epoch 4\n",
      " 25%|██▌       | 5/20 [00:43<02:11,  8.76s/it] Epoch 5\n",
      " 30%|███       | 6/20 [00:52<02:01,  8.66s/it] Epoch 6\n",
      " 35%|███▌      | 7/20 [01:00<01:51,  8.58s/it] Epoch 7\n",
      " 40%|████      | 8/20 [01:08<01:41,  8.48s/it] Epoch 8\n",
      " 45%|████▌     | 9/20 [01:17<01:32,  8.40s/it] Epoch 9\n",
      " 50%|█████     | 10/20 [01:25<01:24,  8.44s/it] Epoch 10\n",
      " 55%|█████▌    | 11/20 [01:33<01:15,  8.37s/it] Epoch 11\n",
      " 60%|██████    | 12/20 [01:42<01:06,  8.30s/it] Epoch 12\n",
      " 65%|██████▌   | 13/20 [01:49<00:57,  8.16s/it] Epoch 13\n",
      " 70%|███████   | 14/20 [01:57<00:47,  7.98s/it] Epoch 14\n",
      " 75%|███████▌  | 15/20 [02:05<00:39,  7.86s/it] Epoch 15\n",
      " 80%|████████  | 16/20 [02:12<00:31,  7.89s/it] Epoch 16\n",
      " 85%|████████▌ | 17/20 [02:20<00:23,  7.91s/it] Epoch 17\n",
      " 90%|█████████ | 18/20 [02:28<00:15,  7.87s/it] Epoch 18\n",
      " 95%|█████████▌| 19/20 [02:36<00:07,  7.92s/it] Epoch 19\n",
      "100%|██████████| 20/20 [02:44<00:00,  8.23s/it]\n",
      "Spam:11980.0 - Ham:77180.0\n",
      "Done\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 386.845312 248.518125\" width=\"386.845312pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2020-12-03T17:14:21.207296</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 386.845312 248.518125 \r\nL 386.845312 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 44.845313 224.64 \r\nL 379.645313 224.64 \r\nL 379.645313 7.2 \r\nL 44.845313 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m523889ffae\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"90.499858\" xlink:href=\"#m523889ffae\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- −0.04 -->\r\n      <g transform=\"translate(75.177202 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.59375 35.5 \r\nL 73.1875 35.5 \r\nL 73.1875 27.203125 \r\nL 10.59375 27.203125 \r\nz\r\n\" id=\"DejaVuSans-8722\"/>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"151.372585\" xlink:href=\"#m523889ffae\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- −0.02 -->\r\n      <g transform=\"translate(136.049929 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"212.245313\" xlink:href=\"#m523889ffae\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 0.00 -->\r\n      <g transform=\"translate(201.1125 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"273.11804\" xlink:href=\"#m523889ffae\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 0.02 -->\r\n      <g transform=\"translate(261.985227 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"333.990767\" xlink:href=\"#m523889ffae\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 0.04 -->\r\n      <g transform=\"translate(322.857955 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_6\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"md66c38f226\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#md66c38f226\" y=\"194.989091\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- −0.04 -->\r\n      <g transform=\"translate(7.2 198.78831)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#md66c38f226\" y=\"155.454545\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- −0.02 -->\r\n      <g transform=\"translate(7.2 159.253764)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#md66c38f226\" y=\"115.92\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.00 -->\r\n      <g transform=\"translate(15.579688 119.719219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#md66c38f226\" y=\"76.385455\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.02 -->\r\n      <g transform=\"translate(15.579688 80.184673)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"44.845313\" xlink:href=\"#md66c38f226\" y=\"36.850909\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.04 -->\r\n      <g transform=\"translate(15.579688 40.650128)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_11\">\r\n    <path clip-path=\"url(#pde57643f44)\" d=\"M 0 0 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 44.845313 224.64 \r\nL 44.845313 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 379.645313 224.64 \r\nL 379.645313 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 44.845313 224.64 \r\nL 379.645313 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 44.845313 7.2 \r\nL 379.645313 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pde57643f44\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"44.845313\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "train_dataframe = pd.read_csv('train_set.csv')\n",
    "nn = NeuralNetwork(len(unique_words), (len(unique_words)+1)//2, 1, 0.01)\n",
    "\n",
    "errors = nn.train(train_dataframe, 20)\n",
    "plt.plot(range(1, len(errors) + 1), errors)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Now we will test the neural network with the test set and finally display its accuracy, presicion and recall"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9802513464991023\n0.9436619718309859\n0.9054054054054054\n[[958, 8], [14, 134]]\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "confusion_matrix = [[0 for i in range(2)] for i in range(2)]\n",
    "\n",
    "for index, row in test_set.iterrows():\n",
    "    #print(row['message'])\n",
    "    input_v = Data.get_inputs_count(row['message'],unique_words)\n",
    "    result = round(nn.feedforward(input_v)[0,0])\n",
    "\n",
    "    if row['label_tag'] == result:\n",
    "         accuracy+=1\n",
    "    confusion_matrix[row['label_tag']][int(result)] +=1\n",
    "    #print(row['class'], result,'\\n')\n",
    "\n",
    "accuracy /= len(test_set)\n",
    "presicion = confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[0][1])\n",
    "recall = confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[1][0])\n",
    "\n",
    "print(accuracy)\n",
    "print(presicion)\n",
    "print(recall)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       call      tell     claim     prize     enter    mobile  personal  \\\n",
       "0  0.439445  0.219722  0.219722  0.219722  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.219722  0.219722  0.219722   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "     detail    prompt   careful  \n",
       "0  0.000000  0.000000  0.000000  \n",
       "1  0.219722  0.219722  0.000000  \n",
       "2  0.000000  0.000000  1.098612  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>call</th>\n      <th>tell</th>\n      <th>claim</th>\n      <th>prize</th>\n      <th>enter</th>\n      <th>mobile</th>\n      <th>personal</th>\n      <th>detail</th>\n      <th>prompt</th>\n      <th>careful</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.439445</td>\n      <td>0.219722</td>\n      <td>0.219722</td>\n      <td>0.219722</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.219722</td>\n      <td>0.219722</td>\n      <td>0.219722</td>\n      <td>0.219722</td>\n      <td>0.219722</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.098612</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "tf_idf = Data.tf_idf(\"Call from 08702490080 - tells u 2 call 09066358152 to claim ?5000 prize. U have 2 enter all ur mobile & personal details @ the prompts. Careful!\")\n",
    "pd.DataFrame.from_dict(tf_idf)"
   ]
  },
  {
   "source": [
    "Finally we wil save the model in a pkl file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('Josered30': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dd23ca8811b38879fda96944da089c8da825265b25f70ec47b2de265f93fa09f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}