{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('Josered30': conda)",
   "display_name": "Python 3.8.5 64-bit ('Josered30': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dd23ca8811b38879fda96944da089c8da825265b25f70ec47b2de265f93fa09f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import datetime\n",
    "import math\n",
    "import pickle \n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from collections import OrderedDict\n",
    "from itertools import islice\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate=0.1):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.weights_input_hidden = np.random.uniform(-1,1,size=(hidden_nodes, input_nodes))\n",
    "        self.weights_hidden_output = np.random.uniform(-1,1,size=(output_nodes, hidden_nodes))\n",
    "        self.bias_hidden = np.ones((hidden_nodes,1))\n",
    "        self.bias_output = np.ones((output_nodes,1))\n",
    "        self.learning_rate=learning_rate\n",
    "        \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+ math.exp(-x))\n",
    "    \n",
    "    def derivate(self,x):\n",
    "        return x*(1-x)\n",
    "    \n",
    "   \n",
    "    def feedforward(self,input_v):\n",
    "        sigmoid_vector = np.vectorize(self.sigmoid)\n",
    "        \n",
    "        input_vector = input_v.reshape((self.input_nodes,1))\n",
    "    \n",
    "        hidden = np.dot(self.weights_input_hidden,input_vector)\n",
    "        hidden = np.add(hidden, self.bias_hidden)\n",
    "        hidden = sigmoid_vector(hidden)\n",
    "    \n",
    "        output = np.dot(self.weights_hidden_output, hidden)\n",
    "        output = np.add(output, self.bias_output)\n",
    "        output = sigmoid_vector(output)\n",
    "    \n",
    "        return output\n",
    "    \n",
    "    def backpropagation(self, input_v, target_v):\n",
    "        input_vector = input_v.reshape((self.input_nodes,1))\n",
    "        target_vector = target_v.reshape((self.output_nodes,1))\n",
    "        \n",
    "        sigmoid_vector = np.vectorize(self.sigmoid)\n",
    "        derivate_vector = np.vectorize(self.derivate)\n",
    "    \n",
    "        hidden = np.dot(self.weights_input_hidden,input_vector)\n",
    "        hidden = np.add(hidden, self.bias_hidden)\n",
    "        hidden = sigmoid_vector(hidden)\n",
    "        \n",
    "        output = np.dot(self.weights_hidden_output, hidden)\n",
    "        output = np.add(output, self.bias_output)\n",
    "        output = sigmoid_vector(output)\n",
    "\n",
    "        \n",
    "\n",
    "        output_error = np.subtract(target_vector,output)\n",
    "        error = output_error.sum(0)\n",
    "        \n",
    "        gradient = derivate_vector(output)\n",
    "        gradient = np.multiply(gradient,output_error)\n",
    "        gradient = np.multiply(gradient, self.learning_rate)\n",
    "        \n",
    "        hidden_transpose = np.transpose(hidden)\n",
    "        weights_ho_deltas = np.dot(gradient, hidden_transpose)\n",
    "        \n",
    "        self.weights_hidden_output = np.add(self.weights_hidden_output, weights_ho_deltas)\n",
    "        self.bias_output = np.add(self.bias_output, gradient)\n",
    "        \n",
    "        \n",
    "        transpose_weights_hidden_output = np.transpose(self.weights_hidden_output)\n",
    "        hidden_error = np.dot(transpose_weights_hidden_output, output_error)\n",
    "        \n",
    "    \n",
    "        hidden_gradient = derivate_vector(hidden)\n",
    "        hidden_gradient = np.multiply(hidden_gradient, hidden_error)\n",
    "        hidden_gradient = np.multiply(hidden_gradient, self.learning_rate)\n",
    "        \n",
    "        input_transpose = np.transpose(input_vector)\n",
    "        weights_ih_deltas = np.dot(hidden_gradient, input_transpose)\n",
    "        \n",
    "        self.weights_input_hidden = np.add(self.weights_input_hidden, weights_ih_deltas)\n",
    "        self.bias_hidden = np.add(self.bias_hidden, hidden_gradient)\n",
    "\n",
    "        return error\n",
    "    \n",
    "\n",
    "    def train(self, train_dataframe, epochs):\n",
    "        spam = 0\n",
    "        ham = 0\n",
    "        iteration = 0\n",
    "        error_sample = 200\n",
    "        errors = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "            print(\"Epoch\", i)\n",
    "            for index, row in train_dataframe.iterrows():\n",
    "                spam+=(row['label_tag'])  \n",
    "                input_v = row.to_numpy()\n",
    "                input_v = input_v[1:len(input_v)-1]\n",
    "                target_v = np.array([row['label_tag']])\n",
    "                error = self.backpropagation(input_v, target_v)\n",
    "                \n",
    "                if iteration%error_sample == 0:\n",
    "                    errors.append(error)\n",
    "                    print(\"Iteration\", iteration, \"error\", error)\n",
    "                iteration += 1\n",
    "            print(\"\\n\")\n",
    "            \n",
    "        ham = (len(train_dataframe)*epochs)-spam\n",
    "        print(f\"Spam:{spam} - Ham:{ham}\")\n",
    "        print(\"Done\")  \n",
    "\n",
    "        return np.array(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtil:\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_data(message):\n",
    "        message = re.sub(r\"\\$[\\d]+\",'price',message)\n",
    "        message = re.sub(r\"\\%[\\d]+\",'percentage',message)\n",
    "        message = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",'url',message)\n",
    "        message = re.sub(r\"www.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",'url',message)\n",
    "        message = re.sub(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\",'email',message)\n",
    "        message = re.sub(r'[\\W\\d]',' ',message)\n",
    "        message = re.sub(r'[\\s+]',' ',message)\n",
    "        message = message.strip()\n",
    "        return message\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_data(message):\n",
    "        message = message.lower() \n",
    "        message = DataUtil.normalize_data(message)\n",
    "        words = nltk.word_tokenize(message)\n",
    "\n",
    "        result = []\n",
    "        for word in words:\n",
    "            if word not in DataUtil.stop_words and len(word)>2:   \n",
    "                #words = DataUtil.stemmer.stem(words[i])\n",
    "                word = DataUtil.lemmatizer.lemmatize(word)\n",
    "                result.append(word)  \n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def order_and_take(data, key, n=None):\n",
    "        data = OrderedDict(sorted(data.items(), key=lambda i: i[1][key], reverse=True))\n",
    "        if n!=None:\n",
    "            data = dict(islice(data.items(), n))\n",
    "        return data\n",
    "\n",
    "\n",
    "class DocumentReader:\n",
    "    def __init__(self, document):\n",
    "        self.document = document\n",
    "        self.words_data = {}\n",
    "\n",
    "    def get_words(self):\n",
    "        df = pd.read_csv(self.document)\n",
    "        words_list = dict()\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            words = DataUtil.clean_data(row['message'])\n",
    "            for word in words: \n",
    "                words_list[word] = 0\n",
    "        return words_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    @staticmethod\n",
    "    def tf(sentences):    \n",
    "        words_counter = {}\n",
    "        for index, sentence in enumerate(sentences):\n",
    "            words = DataUtil.clean_data(sentence)\n",
    "            for word in words: \n",
    "                    if word not in words_counter.keys(): \n",
    "                        words_counter[word] = {}\n",
    "                        words_counter[word]['sentences'] = {}\n",
    "                    if index not in words_counter[word]['sentences'].keys():\n",
    "                        words_counter[word]['sentences'][index] = 1/len(words)         \n",
    "                    else:\n",
    "                        words_counter[word]['sentences'][index] += 1/len(words)\n",
    "        return words_counter\n",
    "\n",
    "    @staticmethod\n",
    "    def tf_idf(message):\n",
    "        sentences = nltk.sent_tokenize(message)\n",
    "        words_count = Data.tf(sentences)\n",
    "        words_data = {}\n",
    "\n",
    "        for key, element in words_count.items():\n",
    "            words_data[key] = [0 for i in range(len(sentences))]\n",
    "            idf = math.log(len(sentences)/len(element['sentences']))\n",
    "            for index, sentence_ratio in element['sentences'].items():    \n",
    "                words_data[key][index] = sentence_ratio * idf\n",
    "        return words_data\n",
    "          \n",
    "    @staticmethod\n",
    "    def get_inputs_count(message, words_dict):  \n",
    "        words = DataUtil.clean_data(message)  \n",
    "        inputs = np.zeros(len(words_dict))\n",
    "\n",
    "        for index, key in enumerate(words_dict.keys()):\n",
    "            if key in words:\n",
    "                inputs[index] +=1 \n",
    "        return inputs\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_unique_words(dataframe):\n",
    "        unique_words = {}\n",
    "        for index,row in dataframe.iterrows():\n",
    "            unique_words[row['word']] = 0\n",
    "        return unique_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_words_df = pd.read_csv('words.csv')\n",
    "unique_words = Data.load_unique_words(unique_words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  class                                            message  label_tag\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0\n",
       "1   ham                      Ok lar... Joking wif u oni...          0\n",
       "2   ham  U dun say so early hor... U c already then say...          0\n",
       "3   ham  Nah I don't think he goes to usf, he lives aro...          0\n",
       "4   ham  Even my brother is not like to speak with me. ...          0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>message</th>\n      <th>label_tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "df = pd.read_csv('spamham.csv')\n",
    "df[\"label_tag\"] = df[\"class\"].map({'ham':0, 'spam':1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     class                                            message  label_tag\n",
       "2304   ham                 Why de. You looking good only:-)..          0\n",
       "4901  spam  Today's Offer! Claim ur ?150 worth of discount...          1\n",
       "3756   ham           You still around? I could use a half-8th          0\n",
       "5159  spam  Someone has contacted our dating service and e...          1\n",
       "2511   ham                   Sorry,in meeting I'll call later          0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>message</th>\n      <th>label_tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2304</th>\n      <td>ham</td>\n      <td>Why de. You looking good only:-)..</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4901</th>\n      <td>spam</td>\n      <td>Today's Offer! Claim ur ?150 worth of discount...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3756</th>\n      <td>ham</td>\n      <td>You still around? I could use a half-8th</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5159</th>\n      <td>spam</td>\n      <td>Someone has contacted our dating service and e...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2511</th>\n      <td>ham</td>\n      <td>Sorry,in meeting I'll call later</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "train_set = df.sample(frac=0.8)\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   class                                            message  label_tag\n",
       "4    ham  Even my brother is not like to speak with me. ...          0\n",
       "5    ham  As per your request 'Melle Melle (Oru Minnamin...          0\n",
       "10   ham  Eh u remember how 2 spell his name... Yes i di...          0\n",
       "15   ham  Aft i finish my lunch then i go str down lor. ...          0\n",
       "17   ham  Just forced myself to eat a slice. I'm really ...          0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>message</th>\n      <th>label_tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ham</td>\n      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>ham</td>\n      <td>Eh u remember how 2 spell his name... Yes i di...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>ham</td>\n      <td>Aft i finish my lunch then i go str down lor. ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>ham</td>\n      <td>Just forced myself to eat a slice. I'm really ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "test_set = df.drop(train_set.index)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('nn.pkl', 'rb')\n",
    "nn = pickle.load(output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9865350089766607\n0.9699248120300752\n0.9214285714285714\n[[970, 4], [11, 129]]\n"
     ]
    }
   ],
   "source": [
    "acuaracy = 0\n",
    "confusion_matrix = [[0 for i in range(2)] for i in range(2)]\n",
    "\n",
    "for index, row in test_set.iterrows():\n",
    "    #print(row['message'])\n",
    "    input_v = Data.get_inputs_count(row['message'],unique_words)\n",
    "    result = round(nn.feedforward(input_v)[0,0])\n",
    "\n",
    "    if row['label_tag'] == result:\n",
    "         acuaracy+=1\n",
    "    confusion_matrix[row['label_tag']][int(result)] +=1\n",
    "    #print(row['class'], result,'\\n')\n",
    "\n",
    "acuaracy /= len(test_set)\n",
    "presicion = confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[0][1])\n",
    "recal = confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[1][0])\n",
    "\n",
    "print(acuaracy)\n",
    "print(presicion)\n",
    "print(recal)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "input_v = Data.get_inputs_count(\"PRIVATE! Your 2003 Account Statement for shows 800 un-redeemed S. I. M. points. Call 08718738002 Identifier Code: 48922 Expires 21/11/04\",unique_words)\n",
    "round(nn.feedforward(input_v)[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}